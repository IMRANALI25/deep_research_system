from typing import Union, List, Dict
from agents import (
    Agent,
    Runner,
    handoff,
    OpenAIChatCompletionsModel,
    ModelSettings,
    set_tracing_disabled,
    function_tool,
    RunContextWrapper,
    ItemHelpers,
)

# CONSTANTS VARIABLES DECLARATION AND INITIALIZATION
# BASE URLs
GEMINI_BASE_URL = "https://generativelanguage.googleapis.com/v1beta/openai/"

OPENAI_BASE_URL = ""

# GEMINI MODELS VARIANTS
GEMINI_MODEL_2_5_FLASH = "gemini-2.5-flash"
GEMINI_MODEL_1_5_FLASH = "gemini-1.5-flash"
GEMINI_MODEL_2_5_FLASH_LITE = "gemini-2.5-flash-lite"
GEMINI_MODEL_2_5_PRO = "gemini-2.5-pro"

# OPENAI MODELS
OPENAI_MODEL_4 = "gpt-4"
OPENAI_MODEL_3_5 = "gpt-3.5-turbo"

# USER SELECTED MODEL
SELECTED_MODEL = GEMINI_MODEL_2_5_FLASH

# ğŸ”‘ DECLARE API KEY VARIABLES
GEMINI_API_KEY: str | None
TAVILY_API_KEY: str | None
OPENAI_API_KEY: str | None

# âš™ï¸ LLM SERVICE VARIABLE
LLM_ASYNC_CLIENT = None

# ğŸ² LLM MODEL VARIABLE USING CHAT COMPLETIONS MODEL
LLM_MODEL = None

# ğŸ”£ğŸ› ï¸ MODEL SETTING CLASS PARAMETERS
# ğŸ”¥TEMPERATURE Controls randomness in responses.
# Higher values (e.g., 1.0â€“1.5) â†’ more creative;
# lower values (0â€“0.5) â†’ more deterministic.
TEMPERATURE: float = 0.6
# âš›ï¸ Nucleus sampling. The model considers tokens with cumulative
# probability up to top_p. Typical range: 0â€“1.
TOP_P: float = 0.6
# ğ•„ğ•’ğ•©ğŸ’¯ Maximum number of tokens the model can generate in a single response.
# ğ•„ğ•’ğ•©ğŸ’¯ The range of max_tokens depends on the model: Gemini-2.5-flash / other large models Varies, usually 8192â€“32768
MAX_TOKENS: int = 8192
# âš½ï¸ğŸ¥… Penalizes new tokens based on their existing frequency in the text so far. Range: 0â€“2.
# âŒ Gemini doesnâ€™t support this
FREQUENCY_PENALTY: float = 0.4
# âš½ï¸ğŸ¥… Penalizes new tokens based on whether they appear in the text at all. Range: 0â€“2.
# âŒ Gemini doesnâ€™t support this
PRESENCE_PENALTY: float = 0.4
# ğŸ›‘ List of stop sequences to terminate the modelâ€™s output.
STOP: List[str] = None
# ğŸ’ƒğŸ» Name of the model (e.g., "gpt-4", "gpt-4o-mini", "gpt-3.5-turbo").
MODEL_NAME: str = SELECTED_MODEL
# âœ”ï¸ Which tool the agent can use ("none", "auto", "required","<tool_name>").
TOOL_CHOICE: str = "None"
# â¸â˜ï¸ If True, allows the agent to run multiple tools in parallel.
PARALLEL_TOOL_CALLS: bool = False
# âœ‚ (Optional) Gradually reduce randomness over multiple agent steps.
TEMPERATURE_DECAY: float = 0.0
# ğŸ”„Maximum number of retries if a tool or model call fails.
MAX_RETRIES: int = 1

MODEL_SETTINGS = ModelSettings(
    temperature=TEMPERATURE,
    top_p=TOP_P,
    max_tokens=MAX_TOKENS,
    stop=STOP,
    model_name=MODEL_NAME,
    tool_choice=TOOL_CHOICE,
    parallel_tool_calls=PARALLEL_TOOL_CALLS,
    temperature_decay=TEMPERATURE_DECAY,
    max_retries=MAX_RETRIES,
)
